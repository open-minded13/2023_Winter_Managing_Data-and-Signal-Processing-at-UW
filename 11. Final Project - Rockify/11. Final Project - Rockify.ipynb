{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Project: Rockify\n",
    "\n",
    "This project aims to develop a machine-learning model for music genre classification to identify various rock music sub-genres. Our model, Rockify, is trained on a dataset of rock music from Spotify to classify songs into specific sub-genres, including but not limited to classic rock, alternative, and heavy metal. In this report, we will guide you comprehensively through the methods used, the model, the results, and an in-depth discussion of the implications of this breakthrough in music technology. \n",
    "\n",
    "- Department: Master of Science in Technology Innovation, University of Washington\n",
    "- Team Members: Yvonne Yang, Jiahui Kao, Emily Chou, Chia-Wei Chang"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part A. Rockify's Data Extration\n",
    "\n",
    "The following code uses the Spotify API and the Spotipy library to extract information about the tracks in a Spotify playlist and save it in a CSV file. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spotipy is a Python library for interacting with the Spotify API.\n",
    "import spotipy\n",
    "\n",
    "# SpotifyClientCredentials is a class from spotipy.oauth2 used to \n",
    "# authenticate with the Spotify API using client ID and client secret.\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# csv is a Python library for reading and writing CSV files.\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Load credentials from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_id and client_secret are the unique identifiers provided by Spotify \n",
    "# when you register your application to use the Spotify API.\n",
    "client_id = \"af41dd34ac11488c9351e2632411d974\"\n",
    "client_secret = \"1d69550688494d4c91dbdd5f7ccf769c\"\n",
    "\n",
    "# SpotifyClientCredentials is used to obtain an access token by \n",
    "# providing client_id and client_secret, which is then used to authenticate the spotipy.\n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id=client_id, client_secret=client_secret)\n",
    "\n",
    "# Spotify object, sp, that we will use later in the code.\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Specify the ID of the Spotify playlist you want to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playlist_id is the unique identifier of the Spotify playlist that \n",
    "# we want to extract information from.\n",
    "playlist_id = \"1YC2hYS5awhGQBNaCObjyK\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Get all the tracks in the playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.playlist_tracks() method is used to get a list of all the tracks in the \n",
    "# specified playlist, playlist_id, using the spotipy.Spotify object, sp.\n",
    "tracks = sp.playlist_tracks(playlist_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Create a new CSV file to store the dataset <br/>\n",
    "Step 6. Write the header row of the csv file <br/>\n",
    "Step 7. Loop through each track and extract its information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new file named classic2.csv is created in write mode using the open() method from the csv library.\n",
    "with open(\"Rockify_Dataset.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    \n",
    "    # A csv.writer object is created, which will be used to write data to the CSV file.\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # A list of header row column names is created and written to the CSV file \n",
    "    # using the writerow() method from the csv.writer object created earlier.\n",
    "    header = ['Track ID', 'Track Name', 'Artist', 'Album', 'Duration (ms)', \n",
    "              'Dance Ability', 'Energy', 'Key', 'Loudness', 'Mode', 'Speechiness', \n",
    "              'Acousticness', 'Instrumentalness', 'Liveness', 'Valence', 'Tempo']\n",
    "    \n",
    "    # Write the header row of the csv file.\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Loop through each track and extract its information.\n",
    "    for track in tracks['items']:\n",
    "       \n",
    "       ## Get the track information.\n",
    "       \n",
    "       # track_id: The unique ID of the track in the Spotify database.\n",
    "       track_id = track['track']['id']    \n",
    "       # track_name: The name of the track.\n",
    "       track_name = track['track']['name']\n",
    "       # artist: The name of the artist.\n",
    "       artist = track['track']['artists'][0]['name']\n",
    "       # album: The name of the album the track is from.\n",
    "       album = track['track']['album']['name']\n",
    "       # duration_ms: The duration of the track in milliseconds.\n",
    "       duration_ms = track['track']['duration_ms']\n",
    "\n",
    "       # Use the track_id to obtain the audio features for the track \n",
    "       # using the sp.audio_features() function. \n",
    "       audio_features = sp.audio_features(track_id)[0]\n",
    "       \n",
    "       ## Extract the audio features.\n",
    "       \n",
    "       # danceability: A value representing how suitable the track is \n",
    "       #               for dancing based on a combination of musical elements.\n",
    "       danceability = audio_features['danceability']\n",
    "       # energy: A value representing the intensity and activity level of the track.\n",
    "       energy = audio_features['energy']\n",
    "       # key: The musical key the track is in.\n",
    "       key = audio_features['key']\n",
    "       # loudness: The overall loudness of the track in decibels.\n",
    "       loudness = audio_features['loudness']\n",
    "       # mode: Whether the track is in a major or minor key.\n",
    "       mode = audio_features['mode']\n",
    "       # speechiness: A value representing the amount of spoken word in the track.\n",
    "       speechiness = audio_features['speechiness']\n",
    "       # acousticness: A value representing how acoustic the track is.\n",
    "       acousticness = audio_features['acousticness']\n",
    "       # instrumentalness: A value representing how instrumental the track is.\n",
    "       instrumentalness = audio_features['instrumentalness']\n",
    "       # liveness: A value representing the presence of a live audience in the recording.\n",
    "       liveness = audio_features['liveness']\n",
    "       # valence: A value representing the musical positiveness conveyed by a track.\n",
    "       valence = audio_features['valence']\n",
    "       # tempo: The overall tempo or speed of the track in beats per minute.\n",
    "       tempo = audio_features['tempo']\n",
    "\n",
    "       # Write the row to the CSV file.\n",
    "       row = [track_id, track_name, artist, album, duration_ms, \n",
    "              danceability, energy, key, loudness, mode, speechiness, \n",
    "              acousticness, instrumentalness, liveness, valence, tempo]\n",
    "       writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part B. Rockify's ML Training Models & Evaluations\n",
    "\n",
    "The following code builds and evaluates various classifier models using the extracted data provided from Part A. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in Step 2-4\n",
    "import pandas as pd\n",
    "\n",
    "# Used in Step 4\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Used in Step 5\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Used in Step 6\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Used in Step 6-7\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Used in Step 6-11\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Used in Step 7\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Used in Step 8\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Used in Step 9\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Used in Step 10\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Load the dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'Rockify_Dataset.csv' file from the local system using \n",
    "# the pandas library's read_csv function and stores it in the data variable.\n",
    "data = pd.read_csv(\"/Rockify_Dataset.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Cleaning non-numeric data\n",
    "\n",
    "The code drops the 'Track Name' column using the pandas drop function and removes any non-numeric values from the 'Genre' column by converting them to numeric values using a dictionary. **This step ensures that the 'Genre' column contains only numeric values, which is necessary for the machine learning algorithms used later in the code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Track Name.\n",
    "data = data.drop(['Track Name'], axis=1)\n",
    "\n",
    "# Next, we convert the values in the 'Genre' column, which is a categorical variable, \n",
    "# to numeric values. It first obtains the unique values of the 'Genre' column and converts \n",
    "# them to a list using the unique and tolist functions, respectively.\n",
    "genre_values = data['Genre'].unique().tolist()\n",
    "\n",
    "# We then create a dictionary that maps each genre to a unique integer value. \n",
    "# The for loop iterates through the values in the genre_values list and \n",
    "# assigns each value to a unique integer. The resulting dictionary is stored \n",
    "# in the genre_dic variable.\n",
    "genre_dic = {}\n",
    "for i in range(len(genre_values)):\n",
    "    genre_dic[genre_values[i]] = i\n",
    "\n",
    "# Finally, the map function of the Pandas library is used to apply the dictionary to \n",
    "# the 'Genre' column, converting each genre value to its corresponding integer value.\n",
    "data['Genre'] = data['Genre'].map(genre_dic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Visualizing variable correlation\n",
    "\n",
    "The code creates a heatmap of the variable correlation between different columns in the data using the seaborn library's heatmap function and visualizes it using the plt.show() function. The purpose of visualizing the variable correlation is to gain insights into the relationships between the different features in the dataset. The code generates a heatmap of the correlation matrix using the seaborn library, which provides a graphical representation of the pairwise correlations between the features.\n",
    "\n",
    "The resulting plot provides a visual representation of the correlation between each pair of features in the dataset, with lighter colors indicating stronger positive correlations and darker colors indicating stronger negative correlations. **By examining the heatmap, we can identify which features are most strongly correlated with each other, which can be useful for feature selection and model building.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# We use the sns.heatmap() function to create a heatmap of he correlation matrix, \n",
    "# passing in the data.corr() function as an argument to \n",
    "# compute the correlation between each pair of columns. \n",
    "# The annot=True parameter is used to display the correlation coefficients in each cell.\n",
    "# The cmap parameter is used to set the color map for the heatmap.\n",
    "sns.heatmap(data.corr(), annot=True, cmap='RdYlGn', linewidths=0.2)\n",
    "\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. Splitting the data:\n",
    "\n",
    "The code splits the data into training and testing sets using the train_test_split function from the scikit-learn library.\n",
    "\n",
    "- Features: These are the **input variables** or independent variables that are used to make predictions or **to train the model**. \n",
    "- Target: This is the **output variable** or dependent variable that **we want to predict** or classify.\n",
    "\n",
    "The number 42 is a reference to the book \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams. In the book, a group of hyper-intelligent beings ask a supercomputer named Deep Thought to calculate the answer to the ultimate question of life, the universe, and everything. After much anticipation, Deep Thought finally reveals that the answer is 42, but the characters are disappointed because they don't know what the question is.\n",
    "\n",
    "The use of 42 as the value for random_state in the train_test_split() function is a playful reference to this book and has become a convention among many data scientists and machine learning practitioners. However, in practice, any non-negative integer value can be used for random_state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y are the features and target variables respectively.\n",
    "\n",
    "X = data.drop(['Genre'], axis=1)\n",
    "y = data['Genre']\n",
    "\n",
    "# test_size = 0.2 specifies that 20% of the data should be used for testing.\n",
    "# random_state = 42 ensures that the same random split is obtained each time the code is run.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps, we are going to implement Random Forest, Decision Tree, KNN, SVM, and Logistic Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Building and Evaluating: Decision Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A DecisionTreeClassifier() model is initialized with the default hyperparameters.\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# The model is trained using the training data using the .fit() function.\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# The predictions for the test data are then made using the .predict() function, \n",
    "# and these predictions are stored in the y_pred variable.\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "## Evaluate the model.\n",
    "\n",
    "# The accuracy_score() function calculates the accuracy of the model on the test data, \n",
    "# which is the fraction of correctly classified examples over the total number of examples.\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# The confusion_matrix() function returns a confusion matrix that summarizes the number of \n",
    "# true positives, true negatives, false positives, and false negatives for each class.\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# The classification_report() function returns a string that summarizes \n",
    "# the classification metrics for each class in a tabular format. \n",
    "# The metrics include precision, recall, f1-score, and support\n",
    "# (the number of samples in each class).\n",
    "print('Classfication Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7-1. Building and Evaluating: Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random forest classifier model is created with 100 estimators and a maximum depth of 10.\n",
    "rfc_model = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "\n",
    "# The model is trained using the training data using the .fit() function.\n",
    "rfc_model.fit(X_train, y_train)\n",
    "\n",
    "# The model is used to predict the genre of the test data (X_test). \n",
    "# The predicted genre values are stored in y_pred.\n",
    "y_pred = rfc_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Classfication Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7-2. Perform a grid search to find the best hyperparameters for the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The param_grid dictionary contains a set of values to be tested for the hyperparameters \n",
    "# such as the number of trees in the forest (n_estimators), \n",
    "# the maximum depth of each tree (max_depth), \n",
    "# the minimum number of samples required to split a node (min_samples_split), \n",
    "# the minimum number of samples required to be at a leaf node (min_samples_leaf), and \n",
    "# the method of sampling data (bootstrap).\n",
    "param_grid = {'n_estimators': [100, 200, 300, 400, 500],\n",
    "              'max_depth': [5, 10, 15, 20, 25, None],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'bootstrap': [True, False]}\n",
    "\n",
    "model_for_grid_search = RandomForestClassifier()\n",
    "\n",
    "# The GridSearchCV function from the scikit-learn library performs the grid search \n",
    "# by fitting the model with each set of hyperparameters and \n",
    "# evaluating its performance using cross-validation.\n",
    "# The cv parameter specifies the number of folds for the cross-validation.\n",
    "grid_search = GridSearchCV(model_for_grid_search, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Once the grid search is complete, the best hyperparameters are \n",
    "# returned using the best_params_ attribute.\n",
    "n_estimators, max_depth, min_samples_split, min_samples_leaf, bootstrap = grid_search.best_params_.values()\n",
    "\n",
    "# Retrain the model using optimal parameters.\n",
    "model = RandomForestClassifier(n_estimators, max_depth, min_samples_split, min_samples_leaf, bootstrap)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Classfication Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8. Building and Evaluating: K-Nearest Neighbors Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A K-nearest neighbors classifier model is created with 5 neighbors.\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# The model is trained using the training data using the .fit() function.\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# The model is used to predict the genre of the test data (X_test). \n",
    "# The predicted genre values are stored in y_pred.\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Classfication Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9. Building and Evaluating: Support Vector Machine (SVM) Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A support vector machine (SVM) classifier model is created with \n",
    "# a radial basis function (RBF) kernel, \n",
    "# a regularization parameter of 1.0, and \n",
    "# an auto value for the gamma parameter. \n",
    "svm_model = SVC(kernel='rbf', C=1.0, gamma='auto')\n",
    "\n",
    "# The model is trained using the training data using the .fit() function.\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Classfication Report:', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10. Building and Evaluating: Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using logistic regression models to predict Genre.\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model.\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, y_pred))\n",
    "print('Classfication Report:', classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
